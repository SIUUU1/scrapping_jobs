from playwright.sync_api import sync_playwright
import time
from bs4 import BeautifulSoup
import csv

# all_jobs = []
keywords = ["flutter", "nextjs", "kotline"]

class Job:
  def __init__(self, link, title, company, experience, reward):
    self.link = link
    self.title = title
    self.company = company
    self.experience = experience
    self.reward = reward

  def __str__(self):
    return f"link: {self.link}, title: {self.title}, company: {self.company}, location: {self.location}, reward: {self.reward}"
  
  def get_att_list(self):
    return [self.link, self.title, self.company, self.experience, self.reward]

#scroll_to_bottom 
def scroll_to_bottom(page, delay=1, max_tries=30):
    last_height = 0

    for i in range(max_tries):
        # End 키를 눌러 스크롤 내리기
        page.keyboard.press("End")
        time.sleep(delay)

        # 현재 문서 높이 가져오기
        new_height = page.evaluate("document.body.scrollHeight")

        # 더 이상 스크롤이 안 내려갈 때 종료
        if new_height == last_height:
            print(f"스크롤 완료 (총 {i+1}회 시도)")
            break
        last_height = new_height

    else:
        print("최대 시도 횟수 도달 (페이지가 너무 길거나 무한스크롤일 수 있음)")

#content 가져오기
def get_content(keyword):
  p = sync_playwright().start()

  browser = p.chromium.launch(headless=False)

  page = browser.new_page()

  #page.goto("https://www.wanted.co.kr")
  url = f"https://www.wanted.co.kr/search?query={keyword}&tab=position"
  page.goto(url)

  # time.sleep(3)
  # #page.screenshot(path="screenshot.png")
  # page.click("button.Aside_searchButton__Ib5Dn")
  # time.sleep(5)
  # page.get_by_placeholder("검색어를 입력해 주세요.").fill("flutter")
  # time.sleep(5)
  # page.keyboard.down("Enter")
  # time.sleep(10)
  # page.click("a#search_tab_position")

  # for x in range(5):
  #   time.sleep(5)
  #   page.keyboard.down("End")

  # time.sleep(5)

  scroll_to_bottom(page)

  content = page.content()

  p.stop()
  return content

#get_jobs
def get_jobs(keyword):
  all_jobs = []
  content = get_content(keyword)
  soup = BeautifulSoup(content, "html.parser")

  jobs = soup.find_all("div", class_="JobCard_container__zQcZs JobCard_container--variant-card___dlv1")

  for job in jobs:
    link = f"https://www.wanted.co.kr{job.find('a')['href']}"
    #https://www.wanted.co.kr/wd/315996
    title = job.find("strong", class_="JobCard_title___kfvj").text
    company = job.find("span", class_="CompanyNameWithLocationPeriod_CompanyNameWithLocationPeriod__company__ByVLu").text
    experience = job.find("span", class_="CompanyNameWithLocationPeriod_CompanyNameWithLocationPeriod__location__4_w0l").text
    reward = job.find("span", class_="JobCard_reward__oCSIQ").text
    
    job = Job(link, title, company, experience, reward)
    all_jobs.append(job)

  # for x in all_jobs:
  #   print(x)

  file = open(f"{keyword}.csv", "w")
  writter = csv.writer(file)
  writter.writerow(["Link", "Title", "Company", "Experience", "Reward"])

  for x in all_jobs:
    writter.writerow(x.get_att_list())


for keyword in keywords:
  get_jobs(keyword)
